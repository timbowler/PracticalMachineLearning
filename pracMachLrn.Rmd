---
title: "Practical Machine Learning Course Project"
author: "Tim Bowler"
date: "April 20, 2017"
output: github_document
---
## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here:

- [Weight Data](http://groupware.les.inf.puc-rio.br/har)

(see the section on the Weight Lifting Exercise Dataset)

## Environment Prep and Data Loading

R libraries required for the analyis are loaded.

```{r gloalOptions, echo = TRUE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, warning = FALSE, message = FALSE)
```

```{r libraries}
library(caret, quietly = TRUE); library(rpart, quietly = TRUE); library(rpart.plot, quietly = TRUE)
library(rattle, quietly = TRUE); library(randomForest, quietly = TRUE); library(corrplot, quietly = TRUE)
library(gbm, quietly = TRUE); library(survival, quietly = TRUE); library(splines, quietly = TRUE)
library(parallel, quietly = TRUE); library(plyr, quietly = TRUE)
```

The data required for the analysis is loaded from the following urls and assigned to a Training/Cross Validation variable and a Test Case variable.

```{r dataLoad}
urlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
urlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

trainData <- read.csv(url(urlTrain))
testCase <- read.csv(url(urlTest))
```

The Training data are partitioned into a training set (70%) and a cross-val/test set (30%).

```{r dataPart}
inTrain <- createDataPartition(y = trainData$classe, p = 0.7, list = FALSE)
training <- trainData[inTrain, ]
testing <- trainData[-inTrain, ]
dim(training)
dim(testing)
```

## Data Cleaning

### Remove Near Zero Variance variables

A cursory examination of the data via str and head showed that there are a number of variables that have little to no variability, and thus wouldn't be useful in any model.  These are removed from the training and testing sets.

```{r nearZeroVar}
NZVvariables <- nearZeroVar(training)
training <- training[, -NZVvariables]
testing <- testing[, -NZVvariables]

dim(training)
dim(testing)
```

### Remove NA variables

There are also a number of variables with large numbers of 'NA' values, and the first six variables are ID classifier variables that wouldn't contribute explanatory value.  These are removed via a two-step process.

#### 1. Format variables as numerics

```{r numeric}
for(i in c(8:ncol(training) - 1)){
      training[, i] <- as.numeric(as.character(training[, i]))
      testing[, i] <- as.numeric(as.character(testing[, i]))
}
```

#### 2. Remove NA's and initial six ID variables

```{r nA}
goodVars <- colnames(training[colSums(is.na(training)) == 0])
goodVars <- goodVars[-c(1:6)]

training <- training[, goodVars]
testing <- testing[, goodVars]
dim(training)
dim(testing)
```

The resulting datasets are reduced to 52 predictor variables and the result variable ('classe').

## Exploratory Graph

An exploratory correlation matrix is produced.

```{r corrGraph}
corrMatrix <- cor(x = training[, -53])
corrplot(corr = corrMatrix, method = "color", type = "lower", order = "hclust", 
         hclust.method = "complete", diag = TRUE, tl.cex = 0.5, tl.col = "black")
```

The matrix demonstrates that there are few highly correlated variables among the 52 remaining predictors, so removing them would make little difference in improving model parsimony.

## Prediction Models

Based on the large number of possible predictor variables, three prediction models will be assesed: Decision Tree analysis, the Generalized Boost Model (GBM), and the Random Forest model.  Model accuracy will be the primary criteria to assess model effectiveness, so a confusion matrix and the resulting accuracy will be reported for each.

### Decision Tree

```{r DTmodel}
set.seed(7769)
modDT <- rpart(formula = classe ~ ., data = training, method = "class")
fancyRpartPlot(model = modDT)
```

```{r DTpredict}
predDT2 <- predict(object = modDT, newdata = testing, type = "class")
confuseDT <- confusionMatrix(data = predDT2, reference = testing$classe)
confuseDT$table
confuseDT$overall[1]
plot(confuseDT$table, col = confuseDT$byClass, 
     main = paste("Decision Tree Accuracy: ", round(x = confuseDT$overall[1], digits = 3)))
```

### Generalized Boost Model

```{r GBMmodel}
set.seed(7769)
ctrlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM <- train(form = classe ~ ., data = training, method = "gbm", 
                   trControl = ctrlGBM, verbose = FALSE)

predGBM <- predict(object = modFitGBM, newdata = testing)
confusionGBM <- confusionMatrix(data = predGBM, reference = testing$classe)
confusionGBM$table
confusionGBM$overall[1]
plot(confusionGBM$table, col = confusionGBM$byClass, 
     main = paste("GBM Accuracy: ", round(x = confusionGBM$overall[1], digits = 3)))
```

### Random Forest

```{r RFmodel}
set.seed(7769)
modFitRF <- train(form = classe ~ ., data = training, method = "rf", 
                  trControl = trainControl(method = "cv", number = 5, verboseIter = FALSE))

predRF <- predict(object = modFitRF, newdata = testing)
confusionRF <- confusionMatrix(data = predRF, reference = testing$classe)
confusionRF
plot(confusionRF$table, col = confusionRF$byClass, 
     main = paste("Random Forest Accuracy: ", round(x = confusionRF$overall[1], digits = 3)))
```

## Result

Hierarchical rank-ordering of model accuracy shows that Random Forest is the preferred model.

1. RF Accuracy: `r confusionRF$overall[1]`
2. GBM Accuracy: `r confusionGBM$overall[1]`
3. DT Accuracy: `r confuseDT$overall[1]`

## Test Case Predictions

The Random Forest model applied to the Test Case data results in the following predictions:

```{r TSTCasePred}
predTESTCASE <- predict(object = modFitRF, newdata = testCase)
predTESTCASE
```
